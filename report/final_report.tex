\documentclass[12pt, letterpaper, final]{report}
\usepackage{hyperref}
\usepackage{ctable}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[section]{placeins}

\author{Devin Schwab (dts34)}
\title{EECS 491 Final Project Report \\ 
\vspace{5 mm}
Learning Basis Functions\\ 
\vspace{5 mm}
 An Evaluation of Protovalue Functions\\
 and the\\
 Representation Policy Iteration Algorithm}

\date{December 12, 2012}

\begin{document}

\maketitle

\section{Abstract}

This project focused on a reinforcement learning technique known as
Representation Policy Iteration (RPI). RPI utilizes samples for a
state space to determine basis functions that are used for
approximation. Traditionally these basis functions have been hand
designed by those creating the reinforcement learning. RPI on the
other hand utilizes information about the state space to create an
approximation. The experiments performed with this framework indicate
that it is indeed a useful algorithm and that while in some cases it may require a
large number of samples to properly converge, when it does converge
the space savings are quite large. All of this experimentation was
done with the intention of understanding the RPI framework so that it
could be used in conjunction with Bayesian Reinforcement Learning.

\tableofcontents
\newpage
\listoffigures

\section{Background}

\subsection*{Markov Decision Process}

A Markov Decision Process (MDP) is a formal representation of the
problem of making actions in an environment that is not entirely
controlled by the agent. To solve an MDP an agent must determine a
policy that maximizes its expected reward up to some time in the
future.

An MDP is specified by the tuple $(S, A, P_a(s,s'), R_a(s,s'))$ where
S is the set of states in the environment, A is the set of actions the
agent can perform, $P_a(s,s')$ is the transition model and $R_a(s,s')$
specifies the reward model. \cite{soumya_ray_eecs_2012} \cite{richard_s._sutton_reinforcement_2005}

MDP's have been a major topic of study for a number of years and so a
number of techniques for solving them have been developed. These
include the model based value iteration algorithm and the model free
Q-learning algorithm. Many of the algorithms are derivatives of these
two algorithms.

Regardless of the technique used to solve the MDP in order for the
solution to be considered optimal it must satisfy the Bellman Optimality
criterion. That is

\[
V^{\pi*} (s) = max_a ( R(s, a) + \gamma \Sigma_{s'} P(s, a, s')
V^{\pi*}(s'))
\]

where $V^{\pi*}$ is the optimal value function and $\gamma$ is the
discount factor. \cite{soumya_ray_eecs_2012} \cite{richard_s._sutton_reinforcement_2005} All techniques are trying to solve this criterion
because it is necessary and sufficient for optimality.

The subject
of this project was to examine one such technique known as
Representation Policy Iteration (RPI). And to consider ways in which
it could be incorporated into the Bayesian RL framework.

\subsection*{Q-function Approximation}

In general when using Q-learning or value iteration one generally runs
into the ``curse of dimensionality'' for any real world
problem. For this reason instead of explicitly tracking $Q^*(s,a)$ for all
states and actions $Q^*(s,a)$ is approximated. Often the approximation
is done by hand using heuristics from expert knowledge.

For instance considering the RTS scenario used in the second
programming assignment of this class, that is two armies fighting each
other, some of the basis functions may be: distance to closest enemy, health
of current unit, and health of enemy unit. All of these properties are
computed based on current or previous information. And all of these
values were picked based on human knowledge of RTS games.

The Q function can then be approximated by weighting each function and
combining them linearly using the following equation
\cite{soumya_ray_eecs_2012} \cite{richard_s._sutton_reinforcement_2005}

\[
Q^*(s,a) \approx \hat{Q}^*(s,a) = \Sigma_j \phi_j(s,a)w_j^*
\]

Obviously it would be ideal if these basis functions could be
automatically determined. Also in order to come up with good basis
functions one must already have knowledge about the problem domain
that the reinforcement learning agent is trying to solve. This is a
type of chicken and egg problem. The RPI
framework provides a solution to the problem by automatically creating
a set of basis functions for the domain.

One thing to note about the basis function approximation is that it
 can have limited success. The mapping of
the solution onto the basis functions may break the convergence property Bellman backup
operator. In other words, the mapping may no longer be a contraction
mapping. So when using basis functions as an approximation the
reinforcement learning algorithms are no longer guaranteed to converge.

\subsection*{Learning Basis Functions}

The RPI framework utilizes previous work done by
Mahadevan\cite{sridhar_mahadevan_proto-value_????} \cite{sridhar_mahadevan_representation_????} on Proto-Value Functions (PVFs). Protovalue functions utilize a
lot of the ideas discussed in the field of principle component
analysis. At their basic level proto-value functions are set of
eigenvectors on the representation of some problem's state space. The
goal of PVFs is to accurately represent the problem space while
avoiding the curse of dimensionality. PVFs do this by keeping track of
only the most important features of a problem space.

The PVFs take advantage of the graph operator known as the normalized
graph Laplacian (however, other operators can be
used such as the unnormalized graph Laplacian). The normalized graph
Laplacian is equal to the following $L_n = D^{1/2}LD^{1/2}$ where D is
the degree matrix of the graph and $L=D - A$ where A is the adjacency
matrix. 

By constructing a graph of the state space from a set of
samples, information about the structure of the state space is
represented. The graph laplacian is an operator performed on a
graph that transforms the graph representation into a special matrix
form. This matrix form can be used with eigen vectors and eigen values
to do a form of principal component analysis on the graph. A subset of
these eigen vectors can then be chosen and used as the basis functions.

The eigen vectors of the graph Laplacian form good basis functions for
2 main reasons. The first is that they are orthogonal meaning that
there is no redundancy when the actual value is approximated by being
projected onto these basis functions. Secondly, the graph Laplacian
encodes structural information about the space and the eigen vectors
capture key parts of that information.

\subsection*{Least-Squares Policy Iteration}

Least-Squares Policy Iteration (LSPI) is a technique for solving an
MDP by approximating the value function using a set of basis functions
or features. \cite{lagoudakis_least-squares_2004} LSPI is an offline algorithm. It converges to a policy
based on a set of sample tuples.

Each sample tuple is of the following form

\[
(s, a, r, s')
\]

Where s is the current state, a is the action taken, r is the reward,
and s' is the resulting state. LSPI uses a set of basis functions and
weights just like discussed earlier. When the basis functions and
weights are represented as matrices the equation for the approximate Q
value becomes

\[
\hat{Q}^*(s,a) = \Phi w^*
\]

In this equation $w^*$ is a column vector of k weights, where k is the
number of basis functions contained within $\Phi$. $\Phi$ is an
$|S||A| \times k$ matrix where $|S|$ is the cardinality of the set of
states and $|A|$ is the cardinality of the set of actions. Each row of
$\Phi$ contains the values of all of the basis functions for a single
(s,a) and each column contains the values of a single basis function
for all (s,a). \cite{lagoudakis_least-squares_2004}

The paper that introduces LSPI derives an explict formula that can be
used to solve for the weights. \cite{lagoudakis_least-squares_2004} The formula is the
following

\[ Aw^* = b\]

A and b are approximated using the samples and then the linear
equation is solved explicitly for the weights. This process is
repeated until the policy specified by the weights changes by no more
than $\epsilon$ or the maximum number of iterations is reached.

\subsection*{Representation Policy Iteration (RPI)}

Representation Policy Iteration (RPI) combines the use of PVFs with
the LSPI algorithm to create an offline algorithm that can
approximately solve an MDP. \cite{sridhar_mahadevan_representation_????}

The basic process is to sample the state space, then construct the
PVFs then run the LSPI algorithm. The RPI framework is what was
experimented with in this project.

\section{Experimental Results}

In this section the experiments that were performed are
summarized. The experiments took place in a grid environment. In the
environment there was a single Agent. Throughout the grid spaces goals
and traps were distributed. If an agent stepped on a goal or trap the
episode would end and the agent would get either a positive reward for
a goal or a negative reward for a trap.

The world also had the option of penalizing the agent for each step it
took. Actions could also be probabilistic. If an action succeeded the
agent would move in the direction specified, if an action failed the
agent would remain in the same state. The world had the option to use
either 4 actions (the cardinal directions) or 8 actions (the cardinal
directions plus diagonals).

To make comparison between the different maps easier the following
properties were used for the maps discussed in this section. The
set of 4 actions was used and the probability of each action's success
was 90\%. A discount factor of .8 was used. Goals had a reward of 100
and traps had a reward of -100. The step cost for the agent was 0.

The main maps investigated were the following

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/10x102room.png}
\caption{Simple 10x10 2 Room Map}
\label{10x102roomMap}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/15x152room.png}
\caption{Simple 15x15 2 Room Map}
\label{15x152roomMap}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/maze01_world.png}
\caption{Auto-generated 10x10 Maze}
\label{10x102roomMap}
\end{figure}
\FloatBarrier

These screenshots are from one of the programs developed. Goal states
are represented by green G's, traps are represented by red G's, the
agent is represented by a yellow A, walls are represented by red *'s and
white -'s represent empty states.

\subsection*{Comparison of Value Function to Approximation}

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/paper_example_V_function_comparison_k25_s5000_graph01.png}
\caption{Comparison of Actual Value Function vs Approximated Value
  Function for Simple 10x10 2 Room problem and 25 PVF}
\label{valueVsQ1}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/paper_example_big_V_function_comparison_k75_s5000_graph01.png}
\caption{Comparison of Actual Value Function vs Approximated Value
  Function for Simple 15x15 2 Room problem and 75 PVF}
\label{valueVsQ2}
\end{figure}
\FloatBarrier

Looking at figures \ref{valueVsQ1} and \ref{valueVsQ2} the shape of
the the Value function is clearly seen in the approximated value
function. Especially in the areas close to the goal state. However,
the magnitudes don't quite match the actual magnitudes and the shapes
are not perfect. On the other hand as seen in the colormap, while the
absolute values may not exactly match the value function the relative
values between states seem to line up.

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/maze01_V_comparison_k50_s5000.png}
\caption{Comparison of Actual Value Function vs Approximated Value
  Function for 10x10 Auto-generated maze problem and 50 PVF}
\label{valueVsQ3}
\end{figure}
\FloatBarrier

Looking at figure~\ref{valueVsQ3} a correlation between the
approximated value and the actual value can be seen on the
colormap. But from the one dimensional representation there are many
discrepancies. Likely these discrepancies could be reduced by
increasing both the number of samples used when constructing the basis
functions and weights, as well as by increasing the number of basis
functions used.

\subsection*{Policy Comparison}

While the shape and magnitude of the value function is a very good
approximation the policies vary alot. Graphs of the policy for an
approximated solution are shown next to graphs of the policies for the
actual optimal policy for various numbers of basis functions. The graphs show colors for each state in the
grid. The color key is shown next to the graph.

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/paper_example_policy_comparison_k25_s5000_graph01.png}
\caption{Comparison of Actual Value Function vs Approximated Value
  Function for Simple 10x10 2 Room problem and 25 PVF}
\label{policy1}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/paper_example_big_policy_comparison_k75_graph01.png}
\caption{Comparison of Actual Value Function vs Approximated Value
  Function for Simple 15x15 2 Room problem and 75 PVF}
\label{policy2}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/maze01_policy_comparison_k50_s5000.png}
\caption{Comparison of Actual Value Function vs Approximated Value
  Function for Simple 10x10 2 Auto-generated maze problem and 50 PVF}
\label{policy3}
\end{figure}
\FloatBarrier

Its clear that the policies differ in a lot of places. But does this
mean that the policies are useless? In the experiments run these
policies were generally as good as the value function policy. There
are two possible reasons for this.

The first reason is that there are multiple optimal policies. This
is definitely true for the problems examined. For the room problem
domain, so long as the number of
steps taken from the starting position to the goal are the same in two
different policies, those policies are equal in value.

The second reason is that the inoptimality of the approximation is
local. If that local inoptimality is not encountered very often then
on average the approximated policy will appear equal to the optimal
policy.

However, as discussed later in the problems section, these local
sections of inoptimal policies can create issues.

\subsection*{Examination of Laplacian Basis Functions}

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/paper_example_big_laplacian_graph.png}
\caption{Top 4 Smoothest Normalized Laplacian Basis Functions for
  15x15 2 Room Problem}
\label{laplacianBasis1}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/paper_example_multiroom_laplacian_graph.png}
\caption{Top 4 Smoothest Normalized Laplacian Basis Functions for
  10x10 3 Room Problem}
\label{laplacianBasis2}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/maze01_laplacian_graph.png}
\caption{Top 4 Smoothest Normalized Laplacian Basis Functions for
  10x10 Auto-generated maze Problem}
\label{laplacianBasis3}
\end{figure}
\FloatBarrier

The top 3 smoothest normalized Laplacian basis functions for 3
different maps are shown in figures \ref{laplacianBasis1},
\ref{laplacianBasis2}, and \ref{laplacianBasis3}. Examining each of
them the underlying structure of the map can clearly be seen. Looking
at only the top left graph of each figure a clear delineation between
walls and empty spaces can be made. 

The top right graph in each figure
highlights the section of the state space that is ideal because of its
proximity to the goal as well as the section of the state space that
is least ideal because of its distance from the goal. These two
sections differ in sign so when the weights attempt to converge
theoretically the section close to the goal will be multiplied by a
weight that makes the value large and positive and the other section
will be multiplied by the same weight. However, due to the sign
difference between the two sections the value will come out large and
negative. This information alone highlights the good and bad sections
to the agent.

The other two graphs do similar things to the first two. By combining
these functions with weights an approximate value function can be created.

\subsection*{Convergence Rate}


The properties of RPI's convergence is still an area that needs to
researched. \cite{sridhar_mahadevan_representation_????} For this reason I examined the convergence rate for the
rooms problem domain. In all of these experiments
LSPI was set to run till the $|w_{i}-w_{i-1}| \le 10^{-5}$.

It is interesting to examine the convergence rate of the
Q-value approximation to the final value. To examine this the LSPI was
run on the simple 10x10 2 Room problem. The Q-value approximation was
plotted as a 2D color map for 4 different intermediate policies. This
graph is shown in Fig~\ref{Qconvergence1}

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/debugging1.png}
\caption{Approximate Q Value over LSPI Iterations with 25 PVF for
  10x10 2 Room Problem}
\label{Qconvergence1}
\end{figure}
\FloatBarrier

This particular problem converged fairly quickly. The first initial
policy approximation is the only one that is clearly different. The
number of iterations of LSPI that were performed on various problems
was recorded. For each of these problems the world consisted of a
10x10 grid which a number of walls and a single goal worth +100. The
step cost was zero and the actions were up, down, left and right. The
probability of success for each action was .9 and if the action failed
the agent would remain in the same state. Once the goal was reached
the episode was ended and a new one was started.

Using a random policy approximately 5000 samples were collected for
each environment. the RPI algorithm was then run on these samples. In
each of these problems the number of protovalue functions used was 25,
50 and 75. The maximum number of iterations allowed was 20. On
average the number of iterations of LSPI until convergence was 12.7

The time to actually compute these representations obviously varies
based on the number of samples, the size of the world, the number of
actions and the number of basis functions. However, the number of
iterations of LSPI that needed to be performed using the basis
functions were quite few. 


\subsection*{Value Function Approximation Efficiency}

To determine how well the RPI framework functions it is important to
have a good understanding of the algorithms efficiency. Efficiency can
mean multiple things, but for this section when efficiency is
mentioned it is meant to be the following quantity

\[
\eta = \frac{\text{Space Savings}}{\text{Value Function Error}}
\]

This metric is designed to calculate efficiency only in respect to the
approximation of the value function, not the policy or any other
property. The reason this metric is used is because both space savings and the
deviation from the actual value play a large role. For any algorithm
that attempts to calculate the value function, the algorithm is only
useful if the space savings don't cause a large increase in error. 

The bottom term will always remain the same as the value function
error is simply

\[
| V^{*} - \hat{V}^{*}|
\]

The top term, however, depends on the algorithm this method is being
compared to. For instance the space necessary to store all of the
values for Q-learning explicitly is $|S| \times |A|$ for the reward
function, R(s,a), and $|S|\times|A|$ for the Q function, Q(s,a). Where $|S|$ is
the cardinality of the set of states and $|A|$ is the cardinality of
the set of actions.

On the other hand the storage space of the RPI framework algorithm is 
$k|S|+k$ where k is the number of basis functions and $|S|$ is the
cardinality of the set of states.

Therefore for this project the efficiency metric is

\[
\eta = \frac{( 2|S||A|) - (k|A| + k)}{|V^{*} - \hat{V}^{*}|}
\]

With this metric if the value function error remains constant as the
space savings increase the efficiency metric will increase and vice
versa. Likewise if the space savings remain constant but the value
function error increases then the efficiency will decrease. 

It is
important to note that while this quantity is called efficiency it is
not a percentage like in the traditional sense. Also this metric will
vary from problem to problem. But it is still useful for comparing
problems in a similar domain.

Using this metric a number of room problems were evaluated with both
value iteration and the the RPI algorithm. The number of PVF's was
determined by when the average steps per episode leveled off. An
example graph is shown in Fig~\ref{performanceComparisonGraph1}. In this
case that happened at around 40 episodes of sampling.

\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{images/paper_example_big_comparison_graph02.png}
\caption{Performance Comparison Graph between different PVF's for the
  15x15 2 Room Example}
\label{performanceComparisonGraph1}
\end{figure}
\FloatBarrier

The efficiency metrics for each of the PVFs are as follows

\FloatBarrier
\begin{table}[h!]
  \begin{tabular}{|l|l|l|l|}
  \hline
  {\bf k} & {\bf Space Savings} & {\bf Value Function Error} & $\eta$\\ \hline
  25 & 675 & 231.19 & 2.92 \\ \hline
  50 & 550 & 262      & 2.10  \\ \hline
  75 & 425 &  276.8 & 1.54 \\ \hline
\end{tabular}
\end{table}
\FloatBarrier

As can be seen in the table for this particular experiment 25 basis
functions is the most efficient in terms of reproducing the actual
Value function. This does not necessarily say that 25 basis functions
best represents the policy, as the policy is mostly affected by
ordering and the norm of the value function error says nothing about
the ordering at each state. Still this is useful for illustrating how
a large number of dimensions can be eliminated without the error
becoming extremely large. 

\subsection*{Problems}

While the experiments performed using the RPI framework indicate that
the algorithm does work, in general there are a number of problems with
it.

First off, the quality of the approximated value function is greatly
affected by the area of the state space that is sampled. If there are
areas of the state space that aren't often visited and if these areas
are missed in the sampling step then the basis functions will be based
off an incorrect interpretation of the state space. As seen in the
laplacian basis functions section the basis functions are strongly
correlated to the spatial properties of the state space. This means
that any missed states can possibly drastically change the
approximated value function.

The solution to this is to sample uniformly over the state space and
to collect a large enough group of samples that with reasonable
confidence one can assume no states were missed.

Another problem is that as shown while the algorithm will converge
relatively quickly the approximation of the value function can be
quite inaccurate. This leads to discrepancies in the policies. As seen
in the policies comparison section while the Value function
approximation may look very similar to the actual Value function the
small discrepancies can lead to widly different policies (although not
necessarily incorrect ones). This is due to the policy being based on
ordering of action values vs the value simply being based on the max action.

During the experimentation there were
instances when the agent would get stuck in a particular state or in a
loop of states. In general increasing the samples and the number of
basis functions solved these problems. However, it does point out that
while the approximation of the value function is good, it is not
perfect. And while the policy from the approximated value function may
work for the majority of the states, there are still cases where the
policy can fail.

\section{Future Work}

The simplest extension to this project would be to test the RPI
framework on different problem domains. Other domains such as
continuous ones are addressed in the RPI paper. It would be useful to
further their work in evaluating the usefulness of this algorithm. It
would also be useful to try to determine the properties that are
likely to lead to a good convergence.

Even without further experiments specifically regarding RPI, now that
the RPI framework has been implemented and is understood, an attempt
can be made to apply the theory of Bayesian RL on
top of it.\footnote{See Appendix B for an overview of what is
  referred to by Bayesian RL} Bayesian RL can be used to solve
the exploration vs exploitation problem. Instead of using Bayesian RL
with Q-learning, the Bayesian RL techniques could be applied to the
RPI framework. This would hopefully allow the learned representations
to be generalized to similar problems in a domain. 

The Bayesian RL could also be used to provide insight into which parts
of a new problem's state space should be sampled first. This would
theoretically reduce the number of samples needed. As when a new
problem from the same domain was encountered the basis
functions and policies could most likely be repaired by sampling from
the areas most likely to reveal new information.

\section{Conclusion}

Based on the experiments conducted for this project it is clear that
the RPI framework provides real value to the field of reinforcement
learning. Not only does it allow an agent to learn appropriate basis
functions it also drastically cuts down on the amount of information
that must be stored.

While the results can vary based on the state space and actions, in
general the algorithm seems to function well, while simultaneously
providing tremendous space savings.

\bibliographystyle{unsrt}
\bibliography{491_project.bib}

\section{Appendix A - Experiment Programs}

A number of experimental programs were created for this project. In
general the programs should be capable of applying to any problem
domain so long as a few functions are implemented for the domain. 

The source code for this project is contained within a Github
repository located at https://github.com/rhololkeolke/eecs\_491\_project

The computer that is running the program requires python, scipy, numpy
and the apgl libraries to function. \cite{numpy_developers_numpy_2012}
\cite{enthought_scipy_2012} These can be installed using the
standard python tools such as easy\_install and pip.  The lspiframework directory will also need to be added to the
computer's PYTHONPATH environment variable.

To run the code the basic usage is 

\vspace{5 mm}
{\bf python rooms.py /path/to/map.yaml}
\vspace{5 mm}

where rooms.py is located in the lspi\_examples directory

\section{Appendix B - Bayesian RL}

As mentioned in the future work section one possible next step is to
combine the RPI framework with something like Bayesian RL in order to
try and generalize the solutions found for specific problems to the
entire problem domain.

\subsection*{Bayesian Reinforcement Learning}

At its most basic level Bayesian Reinforcement Learning (Bayesian RL)
attempts to solve the exploration vs exploitation problem. \cite{dearden_bayesian_1998} The
exploration vs exploitation problem is the problem of how to choose
when to exploit knowledge (i.e. execute the policy) and when to
explore for new knowledge (i.e do something other than the
policy). \cite{wyatt_exploration_1998}

Standard Q-learning solves this problem by doing undirected
exploration using algorithms such as $\epsilon$-greedy exploration and
Boltzmann exploration. \cite{dearden_bayesian_1998} However, both of
these techniques while useful are completely undirected. That is they
are just as likely to ``explore'' a state that is well known as they
are to explore a state that is novel (i.e. has not been seen
before). It would be useful to direct the exploration so that when
exploration should occur the exploration actually adds knowledge
rather than simply reinforcing what is already known.

\subsection*{Bayesian Exploration}

In Bayesian Q-Learning by Dearden et al a metric for determing the
usefulness of a exploring a new state is developed. \cite{dearden_bayesian_1998}This metric is
referred to as the Myopic Value of Perfect Information
(Myopic-VPI). The Myopic-VPI is determined by maintaining a
distribution of the Q-values and then using the expected value of the
distribution to compute the likelihood of learning new, useful
information.

In Model Based Bayesian Exploration, by Dearden et al, this idea of
directed exploration is further refined so that instead of directly
keeping distributions on the Q-values, distributions are kept on the
possible models and then Q-values are derived from these
models. \cite{dearden_model_1999} The distributions used are either
Dirchlet or Sparse Dirchlet. This means that the shape of the
distribution will always be Dirchlet as the Dirchlet is its own
conjugate prior. This makes the updates relatively easy. Additionally
the Dirchlet distribution can easily be found by counting the number
of times a state transition and reward occur in the set of samples.

In both of these techniques after obtaining a prior distribution it is necessary to sample from a
distribution of Q-values which requires solving the MDP that is being
sampled. As mentioned throughout this report solving an MDP with
a tabular mapping of Q-values can be intractable in terms of memory
and storage space.

\subsection*{Bayesian Exploration with RPI}

Combining Bayesian Exploration with RPI would theoretically provide a
number of benefits. By using RPI to solve the MDP's that were being
sampled the intractability of the MDP's can be resolved, albeit at the
cost of approximation. Although as shown in the experiments section
the approximate Value functions are quite close to the actual value
function in many cases.

The information from these sampled MDP's can then be applied to new
unseen problems in the same domain. With the distributions learned
from the sampled MDPs, theoretically, the basis functions can  in a sense be
generalized to the entire problem domain.

The combination of these two algorithms has been started. Currently a
small test example uses the Sparse Dirchlet and standard Dirchlet
distributions to estimate statistics about simple categories. For
instance the code was tested with a 2 category problem which was
supposed to represent coin flips. The coin flip probabilities were
estimated properly within 3 decimal places for less than 100 samples.

Using the code that manipulates these distributions as a basis the
Model Based Bayesian Exploration paper techniques could be
implemented in future work.

\end{document}